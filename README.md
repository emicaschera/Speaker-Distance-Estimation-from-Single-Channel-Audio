# Source Distance Estimation from Single-Channel Audio — Encoder/Preprocessing Study

<img src="images/network.png"/>

## Overview
This repository contains experiments on **continuous source (speaker) distance estimation** from **single-channel audio** in reverberant environments.

The core model is a **CRNN with an attention module** that learns a time–frequency attention map over acoustic features and regresses the speaker–microphone distance. The baseline architecture and the original experimental framework are described in the paper:

- **Speaker Distance Estimation in Enclosures From Single-Channel Audio** (IEEE/ACM TASLP 2024) by *Michael Neri, Archontis Politis, Daniel Aleksander Krause, Marco Carli, Tuomas Virtanen*.

On top of that baseline, this repo includes my complementary study focused on **encoder / preprocessing design choices**:

- **Evaluating Encoder Architectures: Performance Variations in Source Distance Estimation Models** (2025, technical report).

## Acknowledgements / Original Work
This project **builds upon** the methodology and implementation of:

> Michael Neri, Archontis Politis, Daniel Aleksander Krause, Marco Carli, and Tuomas Virtanen,  
> “Speaker Distance Estimation in Enclosures From Single-Channel Audio,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, vol. 32, 2024.

In their work, source distance estimation is formulated as a **regression problem** and addressed with a **convolutional recurrent neural network (CRNN)** enhanced by an **attention module** operating on a stacked representation of:
- STFT magnitude
- sin(phase)
- cos(phase)

The baseline is evaluated across multiple realism levels (synthetic RIR simulation, hybrid measured-RIR convolution, and real recordings) and multiple datasets.

If you use this repository, please make sure to **cite the original paper** (see the Citation section below).

## What I Contributed (My Work)
My contribution is an **ablation and comparative study on the input preprocessing stage**, keeping the downstream architecture (attention + CNN blocks + biGRU + FC regression head) unchanged.

### Goal
Evaluate how different **encoder/filterbank designs** impact distance-estimation performance when used as the feature extraction front-end.

### Encoders Evaluated
I implemented and benchmarked several encoder families, mainly using the **Asteroid** audio library filterbank components, plus a custom PyTorch encoder:

1. **STFT-based encoder (fixed, analytic)** — baseline front-end  
2. **Free (unconstrained) filterbank** — fully learnable filters  
3. **Analytic Free filterbank** — free filters with analyticity/shift-invariance constraints  
4. **Parametric Sinc filterbank** — learnable parameters within a constrained filter family  
5. **Custom convolutional encoder (PyTorch)** — a 4-block Conv1D + BN + ELU + MaxPool encoder designed to increase representational depth upstream of the network

### Experimental Setup (Encoder Study)
- **Task:** continuous distance regression (Mean Absolute Error as main metric)
- **Data:** 2,500 synthetic samples at 16 kHz (10 s each), generated by convolving anechoic speech with simulated RIRs (same synthetic setup described in the baseline line of work)
- **Validation:** 5-fold cross-validation
- **Training:** Adam optimizer, batch size 16, 50 epochs, learning rate scheduling on validation MSE

### Key Findings (Summary)
Across folds, the **STFT encoder consistently outperformed** the learned / parametric alternatives.

| Encoder | CV Mean MAE (m) |
|--------|------------------|
| **STFT** | **0.138** |
| Custom Conv Encoder | 1.170 |
| Free | 1.200 |
| Parametric Sinc | 1.816 |
| Analytic Free | 1.861 |

Additional observations:
- Learnable free filterbanks showed **high variance across folds**, suggesting sensitivity to initialization and/or limited data regime.
- The fixed STFT front-end provided the most stable and accurate representation for this task, likely due to its strong time–frequency priors and shift-invariant properties.

For full methodology, details, and discussion, see my paper (included in this repo):
- `Evaluating_Encoder_Architectures__Performance_Variations_in_Source_Distance_Estimation_Models.pdf`

## Repository Structure (High Level)
- `training_*.py`: training/validation/testing scripts for the approach and scenarios
- `images/network.png`: architecture figure
- `*.pdf`: papers documenting the baseline and the encoder study
- dataset split files (train/val/test) where applicable

> Note: File/folder names can differ depending on how you cloned/exported the project. Adjust paths accordingly.


## Datasets
This project uses multiple datasets for speaker/source distance estimation.

### Provided / Referenced
- **STARS23**
- **QMULTIMIT**
- **Synthetic dataset setup** (simulated RIRs convolved with anechoic speech)
- **VoiceHome-2** can be downloaded from Zenodo:  
  https://zenodo.org/records/1252143

### Download link (datasets and splits)
It is possible to download the datasets from the following link:  
https://uniroma3-my.sharepoint.com/:f:/g/personal/mneri2_os_uniroma3_it/Er6NW6ngIbxPubpMy5PKeFkBdZqGHUsRb64GOTXMV3fcpQ

## Citation
If you use this repository in academic work, please cite:

### Baseline paper (original work)
```bibtex
@article{neri2024speaker,
  title   = {Speaker Distance Estimation in Enclosures From Single-Channel Audio},
  author  = {Neri, Michael and Politis, Archontis and Krause, Daniel Aleksander and Carli, Marco and Virtanen, Tuomas},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume  = {32},
  year    = {2024},
  doi     = {10.1109/TASLP.2024.3382504}
}
